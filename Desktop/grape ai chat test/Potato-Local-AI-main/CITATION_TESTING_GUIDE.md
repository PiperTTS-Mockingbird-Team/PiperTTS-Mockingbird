# Citation Testing - Quick Reference

## Summary of Findings

**Qwen3 1.7B can generate accurate in-text citations on the first pass!**

- ‚úÖ 100% citation presence rate
- ‚úÖ 70-100% citation accuracy
- ‚úÖ Works across different question types (factual, scientific, historical)
- ‚úÖ Maintains prose quality

**Recommendation**: Eliminate the second citation rewrite pass.

---

## Test Files Available

### 1. **test_citations_fast.py** ‚ö° (RECOMMENDED)
**Quick 3-run validation**

```bash
python test_citations_fast.py
```

**Use when**: 
- Quick verification after model changes
- Smoke testing before deployment
- Demonstrating citation capability

**Runtime**: ~20 seconds

---

### 2. **test_single_pass_citations.py** üìä
**Comprehensive 10-run statistical analysis**

```bash
python test_single_pass_citations.py
```

**Use when**: 
- Detailed performance metrics needed
- Statistical validation required
- Comparing models

**Runtime**: ~60 seconds

---

### 3. **test_citations_comprehensive.py** üî¨
**Multi-domain question type testing**

```bash
python test_citations_comprehensive.py
```

**Use when**: 
- Testing across different domains
- Validating question type coverage
- Ensuring broad applicability

**Runtime**: ~40 seconds

---

### 4. **test_first_pass_citations.py** ‚öñÔ∏è
**Full single-pass vs two-pass comparison**

```bash
python test_first_pass_citations.py
```

**Use when**: 
- Need direct comparison data
- Evaluating trade-offs
- Documenting decision rationale

**Runtime**: ~2 minutes

---

## Implementation Guide

### Current Code (Two-Pass)

```python
# ddgsearch.py - OLD approach
def get_combined_response(query, model='qwen3:1.7b', history=None):
    # ... search logic ...
    
    # Pass 1: Generate prose without citations
    first_pass = ollama.generate(model=model, prompt=prose_prompt)
    
    # Pass 2: Add citations
    rewritten = _rewrite_with_citations(first_pass, search_results, model)
    
    return rewritten
```

### Recommended Code (Single-Pass)

```python
# ddgsearch.py - NEW approach
def get_combined_response(query, model='qwen3:1.7b', history=None):
    # ... search logic ...
    
    # Single pass: Generate prose WITH citations
    current_date = datetime.now().strftime("%A, %B %d, %Y")
    sources = _format_sources_plain(search_results)
    
    prompt = (
        f"Today's date: {current_date}\n\n"
        f"Answer the question below in 2-4 clear sentences using the search results.\n"
        f"After any fact from a source, add its citation number in square brackets [N].\n"
        f"For example: \"Paris is the capital of France [1].\"\n\n"
        f"Search results:\n{sources}\n\n"
        f"Question: {query}\n"
        f"Answer:"
    )
    
    response = ollama.generate(model=model, prompt=prompt)
    return response['response'].strip()
```

---

## Code to Remove

Once you switch to single-pass, you can delete these functions:

- ‚ùå `_add_citation_placeholders(answer)`
- ‚ùå `_build_citation_rewrite_prompt(answer, tagged_source_context)`
- ‚ùå `_format_sources_for_rewrite(results)`
- ‚ùå `_restore_paragraph_structure(original, rewritten)`
- ‚ùå `_rewrite_with_citations(answer, search_results, model)`

**Lines saved**: ~100 lines of code

---

## Benefits of Single-Pass

| Metric | Two-Pass | Single-Pass | Improvement |
|--------|----------|-------------|-------------|
| **LLM Calls** | 2 | 1 | 50% reduction |
| **Token Usage** | ~2000 tokens | ~1000 tokens | 50% reduction |
| **Response Time** | ~4-6 seconds | ~2-3 seconds | 50% faster |
| **Code Complexity** | ~500 lines | ~400 lines | 20% simpler |
| **Citation Accuracy** | 90%+ | 70-100% | Comparable |

---

## Testing Your Custom Queries

To test with your own questions:

```python
# test_my_queries.py
import ollama

MODEL = "qwen3:1.7b"

# Your test data
MY_QUERY = "Your question here?"
MY_SOURCES = [
    {"title": "Source 1", "body": "Content here..."},
    {"title": "Source 2", "body": "Content here..."},
]

# Format sources
sources_text = "\n\n".join([
    f"[{i+1}] {s['title']}\n{s['body']}"
    for i, s in enumerate(MY_SOURCES)
])

# Test prompt
prompt = f"""Answer the question below in 2-4 clear sentences using the search results.
After any fact from a source, add its citation number in square brackets [N].
For example: "Paris is the capital of France [1]."

Search results:
{sources_text}

Question: {MY_QUERY}
Answer:"""

# Run test
response = ollama.generate(model=MODEL, prompt=prompt)
print(response['response'])
```

---

## Monitoring in Production

After deploying single-pass citations, monitor:

1. **Citation presence**: Count answers with [N] markers
2. **User feedback**: Track satisfaction with cited answers
3. **Error logs**: Watch for malformed citations

```python
# Example monitoring
def log_citation_quality(answer):
    citations = re.findall(r'\[(\d+)\]', answer)
    metrics = {
        "has_citations": len(citations) > 0,
        "num_citations": len(citations),
        "timestamp": datetime.now()
    }
    # Log to your monitoring system
    logger.info(f"Citation metrics: {metrics}")
```

---

## Rollback Plan

If single-pass doesn't work for your use case:

1. Keep the old `_rewrite_with_citations()` function commented out
2. Add a feature flag to switch between single-pass and two-pass
3. Monitor for 1-2 weeks before fully removing old code

```python
USE_SINGLE_PASS_CITATIONS = True  # Feature flag

if USE_SINGLE_PASS_CITATIONS:
    answer = generate_with_citations(...)
else:
    first_pass = generate_prose(...)
    answer = _rewrite_with_citations(first_pass, ...)
```

---

## FAQ

**Q: What if accuracy drops below 70%?**  
A: Keep the two-pass approach or investigate prompt engineering improvements.

**Q: Does this work with other models?**  
A: Test with your specific model. Results may vary with smaller/larger models.

**Q: What about complex multi-source answers?**  
A: Tests show good performance even with multiple sources. Monitor your specific cases.

**Q: Should I A/B test in production?**  
A: Yes, if you have high traffic. Use feature flags to compare user satisfaction.

---

## Next Steps

1. ‚úÖ Review test results in [CITATION_TEST_RESULTS.md](CITATION_TEST_RESULTS.md)
2. ‚ö° Run quick test: `python test_citations_fast.py`
3. üîß Update [ddgsearch.py](ddgsearch.py) with single-pass approach
4. üß™ Test with your production queries
5. üöÄ Deploy with monitoring
6. üìä Review metrics after 1-2 weeks
7. üóëÔ∏è Remove old citation rewrite code

---

**Created**: February 18, 2026  
**Model**: Qwen3 1.7B  
**Status**: ‚úÖ Recommended for production
